{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a7d59a68-c226-4e53-b4f1-2b9006a2c0ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seed value\n",
    "# Apparently you may use different seed values at each stage\n",
    "seed_value= 0\n",
    "\n",
    "# 1. Set the `PYTHONHASHSEED` environment variable at a fixed value\n",
    "import os\n",
    "os.environ['PYTHONHASHSEED']=str(seed_value)\n",
    "\n",
    "# 2. Set the `python` built-in pseudo-random generator at a fixed value\n",
    "import random\n",
    "random.seed(seed_value)\n",
    "\n",
    "# 3. Set the `numpy` pseudo-random generator at a fixed value\n",
    "import numpy as np\n",
    "np.random.seed(seed_value)\n",
    "\n",
    "# 4. Set the `tensorflow` pseudo-random generator at a fixed value\n",
    "import tensorflow as tf\n",
    "tf.random.set_seed(seed_value)\n",
    "# for later versions: \n",
    "# tf.compat.v1.set_random_seed(seed_value)\n",
    "\n",
    "# # 5. Configure a new global `tensorflow` session\n",
    "# from keras import backend as K\n",
    "# # session_conf = tf.ConfigProto(intra_op_parallelism_threads=1, inter_op_parallelism_threads=1)\n",
    "# # sess = tf.Session(graph=tf.get_default_graph(), config=session_conf)\n",
    "# # K.set_session(sess)\n",
    "# # for later versions:\n",
    "# session_conf = tf.compat.v1.ConfigProto(intra_op_parallelism_threads=1, inter_op_parallelism_threads=1)\n",
    "# tf.compat.v1.set_random_seed(seed_value)\n",
    "# sess = tf.compat.v1.Session(graph=tf.compat.v1.get_default_graph(), config=session_conf)\n",
    "# K.set_session(sess)\n",
    "\n",
    "from pandas import read_csv\n",
    "import pandas as pd\n",
    "import keras\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import r2_score\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "# from numba import jit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "58fbaa94-c72f-401c-a1aa-1e1082fab0f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#This version does (samples, features, timestamps)\n",
    "def reshaper(data,  n_timestamps, n_features):\n",
    "    n_samples = data.shape[0]\n",
    "\n",
    "    result = np.empty((n_samples, n_features, n_timestamps))\n",
    "    for i in range(n_samples):\n",
    "        c=0\n",
    "        for j in range (n_features):\n",
    "            for k in range(n_timestamps):\n",
    "                result[i][j][k]=data[i][c]\n",
    "                c+=1\n",
    "    \n",
    "    return result\n",
    "\n",
    "# #This version does (samples, timestamps, features)\n",
    "# def reshaper(data,  n_timestamps, n_features):\n",
    "#     n_samples = data.shape[0]\n",
    "\n",
    "#     result = np.empty((n_samples, n_features, n_timestamps))\n",
    "#     for i in range(n_samples):\n",
    "#         c=0\n",
    "#         for j in range (n_features):\n",
    "#             for k in range(n_timestamps):\n",
    "#                 result[i][k][j]=data[i][c]\n",
    "#                 c+=1\n",
    "    \n",
    "#     return result\n",
    "\n",
    "def differentiate_column(column):\n",
    "    return column.diff().fillna(0)  # Using .diff() for differencing and filling NaNs with 0\n",
    "\n",
    "def supervisedReframer(df, steps_back=1, steps_forward=1):\n",
    "    new_df = pd.DataFrame()\n",
    "    new_df['Date'] = pd.to_datetime(df['Date'], origin='1899-12-30', unit='D')\n",
    "\n",
    "    # for col in df.columns:\n",
    "    #     if col != \"Date\":\n",
    "    #         df[col] =  differentiate_column(df[col])\n",
    "\n",
    "    shifted_cols = {}\n",
    "    for col in df.columns:\n",
    "        if col != \"Date\":\n",
    "            for shift in range(steps_back, -1, -1):\n",
    "                if shift != 0:\n",
    "                    new_col_name = f\"{col} (t-{shift})\"\n",
    "                else:\n",
    "                    new_col_name = f\"{col}\"\n",
    "                shifted_cols[new_col_name] = df[col].shift(+shift).copy()\n",
    "\n",
    "    \n",
    "    target_col = {}\n",
    "    for steps in range(1, steps_forward+1):\n",
    "        new_col_name = f\"Target (t+{steps})\"\n",
    "        target_col[new_col_name] = df['Adj Close'].shift(-steps).copy()\n",
    "\n",
    "    # Concatenate shifted columns\n",
    "    shifted_df = pd.DataFrame(shifted_cols)\n",
    "    new_df = pd.concat([new_df, shifted_df], axis=1)\n",
    "\n",
    "    # Concatenate target column\n",
    "    target_df = pd.DataFrame(target_col)\n",
    "    new_df = pd.concat([new_df, target_df], axis=1)\n",
    "    \n",
    "    new_df.dropna(inplace=True)\n",
    "\n",
    "    return new_df\n",
    "\n",
    "def TrainValTestSplit(dataframe, backwards_steps=1,forward_steps=1, n_features=1):\n",
    "    forward_steps+=1\n",
    "    df=dataframe.copy()\n",
    "    df['Year'] = df['Date'].dt.year.copy()\n",
    "\n",
    "    # Determine the years for training, validation, and testing\n",
    "    training_years = df['Year'].unique()[:-2]  # All years except the last two\n",
    "    validation_year = df['Year'].unique()[-2]  # Second to last year\n",
    "    testing_year = df['Year'].unique()[-1]  # Last year\n",
    "\n",
    "    # Select columns for X and y\n",
    "    X_columns = df.columns[1:-forward_steps]\n",
    "    y_columns = df.columns[-forward_steps:-1]\n",
    "    \n",
    "    # Filter the data based on the years\n",
    "    X_train = df[df['Year'].isin(training_years)][X_columns].values\n",
    "    y_train = df[df['Year'].isin(training_years)][y_columns].values\n",
    "    \n",
    "    X_val = df[df['Year'] == validation_year][X_columns].values\n",
    "    y_val = df[df['Year'] == validation_year][y_columns].values\n",
    "    \n",
    "    X_test = df[df['Year'] == testing_year][X_columns].values\n",
    "    y_test = df[df['Year'] == testing_year][y_columns].values\n",
    "    \n",
    "    scaler = MinMaxScaler()\n",
    "    \n",
    "    X_train = scaler.fit_transform(X_train)\n",
    "    X_val = scaler.fit_transform(X_val)\n",
    "    X_test = scaler.fit_transform(X_test)\n",
    "    \n",
    "    y_train = scaler.fit_transform(y_train)\n",
    "    y_val = scaler.fit_transform(y_val)\n",
    "    y_test = scaler.fit_transform(y_test)\n",
    "\n",
    "    X_train = reshaper(X_train, backwards_steps+1, n_features)\n",
    "    X_val = reshaper(X_val, backwards_steps+1, n_features)\n",
    "    X_test = reshaper(X_test, backwards_steps+1, n_features)\n",
    "\n",
    "    # (Samples, timestamps, features version)!!!!\n",
    "    # y_train = y_train.reshape((y_train.shape[0], forward_steps-1, 1))\n",
    "    # y_val = y_val.reshape((y_val.shape[0], forward_steps-1, 1))\n",
    "    # y_test = y_test.reshape((y_test.shape[0], forward_steps-1, 1))\n",
    "\n",
    "    # (Samples, features, timestamps) version!\n",
    "    y_train = y_train.reshape((y_train.shape[0], 1, forward_steps-1))\n",
    "    y_val = y_val.reshape((y_val.shape[0], 1, forward_steps-1))\n",
    "    y_test = y_test.reshape((y_test.shape[0], 1, forward_steps-1))\n",
    "    \n",
    "    return X_train, y_train, X_val, y_val, X_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "630c2304-b4ce-4810-96bd-37724b73fc02",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "could not convert string to float: '#DIV/0!'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 17\u001b[0m\n\u001b[0;32m     13\u001b[0m n_features \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[0;32m     15\u001b[0m moved_df \u001b[38;5;241m=\u001b[39m supervisedReframer(df, n_steps_back, n_steps_forward)\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[1;32m---> 17\u001b[0m X_train,y_train,X_val,y_val,X_test,y_test \u001b[38;5;241m=\u001b[39m \u001b[43mTrainValTestSplit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmoved_df\u001b[49m\u001b[43m,\u001b[49m\u001b[43mn_steps_back\u001b[49m\u001b[43m,\u001b[49m\u001b[43mn_steps_forward\u001b[49m\u001b[43m,\u001b[49m\u001b[43mn_features\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     19\u001b[0m memory_usage \u001b[38;5;241m=\u001b[39m moved_df\u001b[38;5;241m.\u001b[39mmemory_usage(deep\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\u001b[38;5;241m.\u001b[39msum() \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1024\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m1024\u001b[39m)  \u001b[38;5;66;03m# Convert bytes to megabytes\u001b[39;00m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMemory usage of DataFrame: \u001b[39m\u001b[38;5;132;01m{:.2f}\u001b[39;00m\u001b[38;5;124m MB\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(memory_usage))\n",
      "Cell \u001b[1;32mIn[2], line 94\u001b[0m, in \u001b[0;36mTrainValTestSplit\u001b[1;34m(dataframe, backwards_steps, forward_steps, n_features)\u001b[0m\n\u001b[0;32m     90\u001b[0m y_test \u001b[38;5;241m=\u001b[39m df[df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mYear\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m==\u001b[39m testing_year][y_columns]\u001b[38;5;241m.\u001b[39mvalues\n\u001b[0;32m     92\u001b[0m scaler \u001b[38;5;241m=\u001b[39m MinMaxScaler()\n\u001b[1;32m---> 94\u001b[0m X_train \u001b[38;5;241m=\u001b[39m \u001b[43mscaler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     95\u001b[0m X_val \u001b[38;5;241m=\u001b[39m scaler\u001b[38;5;241m.\u001b[39mfit_transform(X_val)\n\u001b[0;32m     96\u001b[0m X_test \u001b[38;5;241m=\u001b[39m scaler\u001b[38;5;241m.\u001b[39mfit_transform(X_test)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\utils\\_set_output.py:295\u001b[0m, in \u001b[0;36m_wrap_method_output.<locals>.wrapped\u001b[1;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[0;32m    293\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(f)\n\u001b[0;32m    294\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapped\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m--> 295\u001b[0m     data_to_wrap \u001b[38;5;241m=\u001b[39m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    296\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_to_wrap, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[0;32m    297\u001b[0m         \u001b[38;5;66;03m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[0;32m    298\u001b[0m         return_tuple \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    299\u001b[0m             _wrap_data_with_container(method, data_to_wrap[\u001b[38;5;241m0\u001b[39m], X, \u001b[38;5;28mself\u001b[39m),\n\u001b[0;32m    300\u001b[0m             \u001b[38;5;241m*\u001b[39mdata_to_wrap[\u001b[38;5;241m1\u001b[39m:],\n\u001b[0;32m    301\u001b[0m         )\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\base.py:1098\u001b[0m, in \u001b[0;36mTransformerMixin.fit_transform\u001b[1;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[0;32m   1083\u001b[0m         warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m   1084\u001b[0m             (\n\u001b[0;32m   1085\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThis object (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m) has a `transform`\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1093\u001b[0m             \u001b[38;5;167;01mUserWarning\u001b[39;00m,\n\u001b[0;32m   1094\u001b[0m         )\n\u001b[0;32m   1096\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m y \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1097\u001b[0m     \u001b[38;5;66;03m# fit method of arity 1 (unsupervised transformation)\u001b[39;00m\n\u001b[1;32m-> 1098\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfit_params\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mtransform(X)\n\u001b[0;32m   1099\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1100\u001b[0m     \u001b[38;5;66;03m# fit method of arity 2 (supervised transformation)\u001b[39;00m\n\u001b[0;32m   1101\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfit(X, y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_params)\u001b[38;5;241m.\u001b[39mtransform(X)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\preprocessing\\_data.py:450\u001b[0m, in \u001b[0;36mMinMaxScaler.fit\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    448\u001b[0m \u001b[38;5;66;03m# Reset internal state before fitting\u001b[39;00m\n\u001b[0;32m    449\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()\n\u001b[1;32m--> 450\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpartial_fit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\base.py:1474\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1467\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1469\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1470\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1471\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1472\u001b[0m     )\n\u001b[0;32m   1473\u001b[0m ):\n\u001b[1;32m-> 1474\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\preprocessing\\_data.py:490\u001b[0m, in \u001b[0;36mMinMaxScaler.partial_fit\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    487\u001b[0m xp, _ \u001b[38;5;241m=\u001b[39m get_namespace(X)\n\u001b[0;32m    489\u001b[0m first_pass \u001b[38;5;241m=\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mn_samples_seen_\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 490\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_data\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    491\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    492\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfirst_pass\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    493\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_array_api\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msupported_float_dtypes\u001b[49m\u001b[43m(\u001b[49m\u001b[43mxp\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    494\u001b[0m \u001b[43m    \u001b[49m\u001b[43mforce_all_finite\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mallow-nan\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    495\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    497\u001b[0m data_min \u001b[38;5;241m=\u001b[39m _array_api\u001b[38;5;241m.\u001b[39m_nanmin(X, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m    498\u001b[0m data_max \u001b[38;5;241m=\u001b[39m _array_api\u001b[38;5;241m.\u001b[39m_nanmax(X, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\base.py:633\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[1;34m(self, X, y, reset, validate_separately, cast_to_ndarray, **check_params)\u001b[0m\n\u001b[0;32m    631\u001b[0m         out \u001b[38;5;241m=\u001b[39m X, y\n\u001b[0;32m    632\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m no_val_y:\n\u001b[1;32m--> 633\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[43mcheck_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mX\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mcheck_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    634\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_y:\n\u001b[0;32m    635\u001b[0m     out \u001b[38;5;241m=\u001b[39m _check_y(y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcheck_params)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\utils\\validation.py:997\u001b[0m, in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[0;32m    995\u001b[0m         array \u001b[38;5;241m=\u001b[39m xp\u001b[38;5;241m.\u001b[39mastype(array, dtype, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m    996\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 997\u001b[0m         array \u001b[38;5;241m=\u001b[39m \u001b[43m_asarray_with_order\u001b[49m\u001b[43m(\u001b[49m\u001b[43marray\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43morder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43morder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mxp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mxp\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    998\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m ComplexWarning \u001b[38;5;28;01mas\u001b[39;00m complex_warning:\n\u001b[0;32m    999\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1000\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mComplex data not supported\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(array)\n\u001b[0;32m   1001\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcomplex_warning\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\utils\\_array_api.py:521\u001b[0m, in \u001b[0;36m_asarray_with_order\u001b[1;34m(array, dtype, order, copy, xp)\u001b[0m\n\u001b[0;32m    519\u001b[0m     array \u001b[38;5;241m=\u001b[39m numpy\u001b[38;5;241m.\u001b[39marray(array, order\u001b[38;5;241m=\u001b[39morder, dtype\u001b[38;5;241m=\u001b[39mdtype)\n\u001b[0;32m    520\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 521\u001b[0m     array \u001b[38;5;241m=\u001b[39m \u001b[43mnumpy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43masarray\u001b[49m\u001b[43m(\u001b[49m\u001b[43marray\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43morder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43morder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    523\u001b[0m \u001b[38;5;66;03m# At this point array is a NumPy ndarray. We convert it to an array\u001b[39;00m\n\u001b[0;32m    524\u001b[0m \u001b[38;5;66;03m# container that is consistent with the input's namespace.\u001b[39;00m\n\u001b[0;32m    525\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m xp\u001b[38;5;241m.\u001b[39masarray(array)\n",
      "\u001b[1;31mValueError\u001b[0m: could not convert string to float: '#DIV/0!'"
     ]
    }
   ],
   "source": [
    "n_steps_back = 10 # Ako je 0, to znaci da koristis danas da predvidis sutra\n",
    "n_steps_forward = 10 #Ako je 0, jbg ne predvidjas ista\n",
    "\n",
    "# load dataset\n",
    "dataset = read_csv('CSV Files with Calculated Indicators/JEX.csv', low_memory=False, header=0, index_col=None)\n",
    "\n",
    "df = dataset\n",
    "\n",
    "# df = pd.DataFrame({\n",
    "#     'Date': dataset['Date'], \n",
    "#     'Adj Close': dataset['Adj Close']})\n",
    "\n",
    "n_features = df.shape[1]-1\n",
    "\n",
    "moved_df = supervisedReframer(df, n_steps_back, n_steps_forward).copy()\n",
    "\n",
    "X_train,y_train,X_val,y_val,X_test,y_test = TrainValTestSplit(moved_df,n_steps_back,n_steps_forward,n_features)\n",
    "\n",
    "memory_usage = moved_df.memory_usage(deep=True).sum() / (1024 * 1024)  # Convert bytes to megabytes\n",
    "print(\"Memory usage of DataFrame: {:.2f} MB\".format(memory_usage))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ba2b802-f13c-4a30-8d04-48e06f284a93",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Create the model\n",
    "inputs = keras.layers.Input(shape=(X_train.shape[1], X_train.shape[2]))\n",
    "x = keras.layers.LSTM(50, return_sequences=True)(inputs)\n",
    "x = keras.layers.LSTM(25, return_sequences=True)(x)\n",
    "x = keras.layers.LSTM(10)(x)\n",
    "outputs = keras.layers.Dense(n_steps_forward, activation='linear')(x)\n",
    "\n",
    "# optimizer = keras.optimizers.Adam(learning_rate=0.1)\n",
    "# model.compile(optimizer=optimizer, loss=\"mse\")\n",
    "\n",
    "model = keras.Model(inputs=inputs, outputs=outputs)\n",
    "model.compile(optimizer='adam', loss=\"mse\")\n",
    "model.summary()\n",
    "\n",
    "early_stopping = keras.callbacks.EarlyStopping(\n",
    "    monitor='val_loss',  # Monitor validation loss\n",
    "    patience=5,           # Number of epochs to wait before stopping\n",
    "    restore_best_weights=True  # Restore weights to the best observed during training\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    epochs = 2,\n",
    "    batch_size = 48,\n",
    "    validation_data=(X_val, y_val),\n",
    "    verbose=1,\n",
    "    shuffle=False\n",
    "    # callbacks=[early_stopping]\n",
    ")\n",
    "\n",
    "y_test_reshaped = y_test.reshape((y_test.shape[0], y_test.shape[2]))\n",
    "\n",
    "# estimate the R2 on the test set\n",
    "print(\"LSTM coefficient of determination of the prediction: \", r2_score(y_test_reshaped, model.predict(X_test)))\n",
    "\n",
    "X_test_skimmed = X_test[::n_steps_forward]\n",
    "y_test_skimmed = y_test_reshaped[::n_steps_forward]\n",
    "\n",
    "predictions = model.predict(X_test_skimmed)\n",
    "\n",
    "y_test_skimmed = y_test_skimmed.flatten()\n",
    "\n",
    "predictions = predictions.ravel()\n",
    "\n",
    "graph_df = pd.DataFrame({\n",
    "    'True': y_test_skimmed, \n",
    "    'Predictions': predictions})\n",
    "\n",
    "last_n_dates = df['Date'].tail(len(graph_df)).tolist()\n",
    "graph_df['Date'] = last_n_dates\n",
    "\n",
    "graph_df['Date'] = pd.to_datetime(graph_df['Date'], origin='1899-12-30', unit='D')\n",
    "\n",
    "graph_df['Predictions'] = graph_df['Predictions'].shift(-1)\n",
    "\n",
    "#Plotting\n",
    "x_min = graph_df['Date'].iloc[0]  # Replace with your desired minimum x-axis value\n",
    "x_max = graph_df['Date'].iloc[-1]  # Replace with your desired maximum x-axis value\n",
    "y_min = min(graph_df['True'].min(), graph_df['Predictions'].min())  # Replace with your desired minimum y-axis value\n",
    "y_max = max(graph_df['True'].max(), graph_df['Predictions'].max())  # Replace with your desired maximum y-axis value\n",
    "\n",
    "# Assuming your DataFrame is named df\n",
    "# Replace 'Date', 'Close', and 'Predicted' with your actual column names\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(graph_df['Date'], graph_df['True'], label='Close', linewidth=1)\n",
    "plt.plot(graph_df['Date'], graph_df['Predictions'], label='Predictions', linestyle='dashed', color='red', linewidth=1)\n",
    "\n",
    "# Customize the plot\n",
    "plt.title('Adj Close vs. Predicted Prices')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Price')\n",
    "plt.legend()\n",
    "\n",
    "# Set x-axis and y-axis value ranges\n",
    "plt.xlim(x_min, x_max)\n",
    "plt.ylim(y_min, y_max)\n",
    "\n",
    "plt.grid(False)\n",
    "plt.xticks(rotation=0)\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6956cf5-d352-41eb-b56a-902d254bc998",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize an empty dictionary to store the history\n",
    "combined_history = {}\n",
    "\n",
    "# Check if there is existing history\n",
    "if 'history' in locals():\n",
    "    # If history exists, add it to the combined history\n",
    "    for key in history.history.keys():\n",
    "        combined_history[key] = history.history[key]\n",
    "\n",
    "# Train the model again, assuming `model` is already defined\n",
    "new_history = model.fit(\n",
    "    X_train, y_train,  # Use your new training data\n",
    "    epochs=1,\n",
    "    batch_size=32,\n",
    "    validation_data=(X_val, y_val),  # Use your new validation data\n",
    "    verbose=1,\n",
    "    shuffle=False\n",
    "    # callbacks=[early_stopping]\n",
    ")\n",
    "\n",
    "# Update the combined history with the new history\n",
    "for key in new_history.history.keys():\n",
    "    if key in combined_history:\n",
    "        combined_history[key].extend(new_history.history[key])\n",
    "    else:\n",
    "        combined_history[key] = new_history.history[key]\n",
    "\n",
    "# estimate the R2 on the test set\n",
    "print(\"LSTM coefficient of determination of the prediction: \", r2_score(y_test_reshaped, model.predict(X_test)))\n",
    "\n",
    "# fit model\n",
    "plt.plot(combined_history['loss'], label='train')\n",
    "plt.plot(combined_history['val_loss'], label='validation')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "X_test_skimmed = X_test[::n_steps_forward]\n",
    "y_test_skimmed = y_test_reshaped[::n_steps_forward]\n",
    "\n",
    "predictions = model.predict(X_test_skimmed)\n",
    "\n",
    "y_test_skimmed = y_test_skimmed.flatten()\n",
    "\n",
    "predictions = predictions.ravel()\n",
    "\n",
    "graph_df = pd.DataFrame({\n",
    "    'True': y_test_skimmed, \n",
    "    'Predictions': predictions})\n",
    "\n",
    "last_n_dates = df['Date'].tail(len(graph_df)).tolist()\n",
    "graph_df['Date'] = last_n_dates\n",
    "\n",
    "graph_df['Date'] = pd.to_datetime(graph_df['Date'], origin='1899-12-30', unit='D')\n",
    "\n",
    "graph_df['Predictions'] = graph_df['Predictions'].shift(-1)\n",
    "\n",
    "#Plotting\n",
    "x_min = graph_df['Date'].iloc[0]  # Replace with your desired minimum x-axis value\n",
    "x_max = graph_df['Date'].iloc[-1]  # Replace with your desired maximum x-axis value\n",
    "y_min = min(graph_df['True'].min(), graph_df['Predictions'].min())  # Replace with your desired minimum y-axis value\n",
    "y_max = max(graph_df['True'].max(), graph_df['Predictions'].max())  # Replace with your desired maximum y-axis value\n",
    "\n",
    "# Assuming your DataFrame is named df\n",
    "# Replace 'Date', 'Close', and 'Predicted' with your actual column names\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(graph_df['Date'], graph_df['True'], label='Close', linewidth=1)\n",
    "plt.plot(graph_df['Date'], graph_df['Predictions'], label='Predictions', linestyle='dashed', color='red', linewidth=1)\n",
    "\n",
    "# Customize the plot\n",
    "plt.title('Adj Close vs. Predicted Prices')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Price')\n",
    "plt.legend()\n",
    "\n",
    "# Set x-axis and y-axis value ranges\n",
    "plt.xlim(x_min, x_max)\n",
    "plt.ylim(y_min, y_max)\n",
    "\n",
    "plt.grid(False)\n",
    "plt.xticks(rotation=0)\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
